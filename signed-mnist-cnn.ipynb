{"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, MaxPool2D, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Import the American Sign Language MNIST Dataset hosted on Kaggle \nxtrain = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\")\nxtest  = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\")\n\n#The MNIST Datset is already labeled - extract and save\nytrain = xtrain['label']\nytest  = xtest['label']\n\n#Remove labels from xtrain and xtest\ndel xtrain['label']\ndel xtest['label']\n\n#Now normalize and reshape the data, to fit the CNN and facilitate faster learning\nxtrain = (xtrain/255).values.reshape(-1, 28, 28 ,1)\nxtest  = (xtest/255).values.reshape(-1, 28, 28, 1)\n\n#Perform data augmentation which will artifically grow dataset, allowing CNN to better generalize learning\n#Images were rotated, zoomed, and shifted by up to 15% of the relevant factor (degrees, zoom, width, height respectively)\n#Note that neither horizontal nor vertical flips were employed, due to the specifics of American Sign Language.\naugdata = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.15, # Randomly zoom image \n        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n#Apply data augmentation to the training set\naugdata.fit(xtrain)\n\n#Reducing learning rate of CNN during plateaus to continue progress\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n\n#Generate CNN Model\nmodel = Sequential()\n\n#First CONV Layer with 16 3x3 Kernels with stride of 1, then 2x2 pooling\nmodel.add(Conv2D(16 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\n#Second CONV Layer with 32 3x3 Kernels with stride of 1, then Dropout regularization and 2x2 pooling\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2)) #Dropout for regularization\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\n#Third CONV Layer with 64 3x3 Kernels with stride of 1, then Dropout regularization and 2x2 pooling\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2)) #Dropout for regularization\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\n#Fourth CONV Layer with 128 3x3 Kernels with stride of 1, then Dropout regularization and 2x2 pooling\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\n#Flatten the 3-dimensional tensor into a vector to input into fully-connected layers\nmodel.add(Flatten())\n\n#First Fully-Connected layer with 256 nodes and Dropout regularization\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dropout(0.3))\n\n#Second Fully-Connected layer with 128 nodes and Dropout regularization\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.3))\n\n#Final output layer with softmax function to categorize as one of the 24 classifications from Signed-MNIST set.\nmodel.add(Dense(units = 25 , activation = 'softmax'))\n\n#Compile model using standard adam optimizer. Note that sparse_categorical_crossentropy is used as these are integers, not one-hot vectors.\nmodel.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\n\n#Create developmental set for usage in training by subdividing training set\nxtrain, xdev, ytrain, ydev = train_test_split(xtrain, ytrain, test_size = 0.2, random_state = 42)\n\n#Fit CNN to training set and perform validation with the dev set\nhistory = model.fit(augdata.flow(xtrain,ytrain, batch_size = 128) ,epochs = 25 , validation_data = (xdev, ydev) , \n                    callbacks = [learning_rate_reduction])\n\n#Evaluate CNN accuracy on the final test set\nprint(\"Test Accuracy \" , model.evaluate(xtest,ytest)[1]*100 , \"%\")\n","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}